#!/usr/bin/env python3
"""
Complete End-to-End Test with Full Aggregation
Tests the entire Grid-X workflow including FedAvg aggregation
"""

import requests
import time
import io
import torch
import torch.nn as nn
import pandas as pd

BACKEND_URL = "http://localhost:8000"

def create_tiny_model():
    """Create a tiny PyTorch model for testing"""
    class TinyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Linear(10, 1)
        
        def forward(self, x):
            return self.fc(x)
    
    return TinyModel()

def main():
    print("üß™ Complete End-to-End Test with Aggregation")
    print("=" * 60)
    
    # Step 1: Create test user
    print("\n1Ô∏è‚É£ Creating test user...")
    email = f"fulltest_{int(time.time())}@gridx.com"
    resp = requests.post(f"{BACKEND_URL}/auth/register", json={
        "email": email,
        "password": "test123"
    })
    user_id = resp.json()['id']
    print(f"‚úÖ User created: ID={user_id}")
    
    # Step 2: Create agent owner and register worker
    print("\n2Ô∏è‚É£ Creating worker...")
    agent_email = f"agent_{int(time.time())}@gridx.com"
    requests.post(f"{BACKEND_URL}/auth/register", json={
        "email": agent_email,
        "password": "agent123"
    })
    
    agent_id = f"test_worker_{int(time.time())}"
    resp = requests.post(f"{BACKEND_URL}/agent/register", json={
        "id": agent_id,
        "email": agent_email,
        "gpu_model": "Test GPU",
        "ram_total": "16GB"
    })
    print(f"‚úÖ Worker registered: {agent_id}")
    
    # Step 3: Prepare tiny training job
    print("\n3Ô∏è‚É£ Preparing tiny PyTorch training job...")
    
    # Training code that creates a simple model
    train_code = """import torch
import torch.nn as nn
import pandas as pd

# Load data
df = pd.read_csv('data.csv')
print(f"Training on {len(df)} rows")

# Create tiny model
class TinyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.fc(x)

model = TinyModel()

# Simple "training" (just initialize)
print(f"Model has {sum(p.numel() for p in model.parameters())} parameters")

# Save model
torch.save(model.state_dict(), 'model.pth')
print("Model saved!")
"""
    
    requirements = "torch\npandas\n"
    
    # Create tiny CSV (20 rows, 10 features)
    df = pd.DataFrame({f'f{i}': range(20) for i in range(10)})
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    
    print(f"   Training code: {len(train_code)} bytes")
    print(f"   CSV data: {len(df)} rows, {len(df.columns)} columns")
    
    # Step 4: Upload job
    print("\n4Ô∏è‚É£ Uploading job...")
    files = {
        'file_code': ('train.py', train_code, 'text/x-python'),
        'file_req': ('requirements.txt', requirements, 'text/plain'),
        'file_data': ('data.csv', csv_buffer.getvalue(), 'text/csv')
    }
    data = {'title': 'Full Aggregation Test', 'user_id': user_id}
    
    resp = requests.post(f"{BACKEND_URL}/jobs/upload", files=files, data=data)
    if resp.status_code != 200:
        print(f"‚ùå Job upload failed: {resp.status_code}")
        print(f"Response: {resp.text}")
        return False
    job_id = resp.json()['job_id']
    print(f"‚úÖ Job uploaded: ID={job_id}")
    
    # Step 5: Wait for CSV splitting
    print("\n5Ô∏è‚É£ Waiting for CSV splitting (10 seconds)...")
    time.sleep(10)
    print("‚úÖ CSV should be split into 5 subtasks")
    
    # Step 6: Process ALL available subtasks
    print("\n6Ô∏è‚É£ Processing all available subtasks...")
    completed_tasks = 0
    max_attempts = 10  # Safety limit
    
    for attempt in range(max_attempts):
        print(f"\n   Requesting task (attempt {attempt+1})...")
        
        # Request task
        resp = requests.post(f"{BACKEND_URL}/agent/request_task", json={
            "agent_id": agent_id
        })
        
        if resp.status_code != 200:
            print(f"   ‚ùå Failed to request task: {resp.text}")
            break
        
        task_data = resp.json()
        if task_data['task_id'] is None:
            print(f"   ‚úÖ No more tasks available - all subtasks processed!")
            break
        
        task_id = task_data['task_id']
        print(f"   ‚Üí Task {task_id} assigned")
        
        # Heartbeat
        requests.post(f"{BACKEND_URL}/agent/heartbeat", json={
            "id": agent_id,
            "status": "BUSY"
        })
        
        # Create dummy model result
        model = create_tiny_model()
        model_bytes = io.BytesIO()
        torch.save(model.state_dict(), model_bytes)
        model_bytes.seek(0)
        
        # Upload result
        files = {'file': ('model.pth', model_bytes, 'application/octet-stream')}
        data = {'agent_id': agent_id, 'task_id': task_id}
        resp = requests.post(f"{BACKEND_URL}/agent/upload_result", files=files, data=data)
        
        if resp.status_code != 200:
            print(f"   ‚ùå Upload failed: {resp.text}")
            break
        
        result_url = resp.json()['url']
        
        # Complete task
        resp = requests.post(f"{BACKEND_URL}/agent/complete_task", json={
            "agent_id": agent_id,
            "task_id": task_id,
            "result_url": result_url
        })
        
        if resp.status_code != 200:
            print(f"   ‚ùå Task completion failed: {resp.text}")
            break
        
        completed_tasks += 1
        print(f"   ‚úÖ Task {task_id} completed ({completed_tasks} total)")
        
        # Mark agent idle
        requests.post(f"{BACKEND_URL}/agent/heartbeat", json={
            "id": agent_id,
            "status": "IDLE"
        })
        
        # Small delay between tasks
        time.sleep(1)
    
    # Step 7: Wait for aggregation
    print(f"\n7Ô∏è‚É£ Waiting for aggregation (5 seconds)...")
    time.sleep(5)
    
    # Step 8: Check job status
    print("\n8Ô∏è‚É£ Checking job status...")
    resp = requests.get(f"{BACKEND_URL}/jobs/list/{user_id}")
    jobs = resp.json()
    
    test_job = next((j for j in jobs if j['id'] == job_id), None)
    
    if test_job:
        print(f"   Job {job_id}:")
        print(f"   Status: {test_job['status']}")
        print(f"   Final result: {test_job.get('final_result_url', 'None')}")
        
        if test_job['status'] == 'COMPLETED':
            print("\n‚úÖ AGGREGATION SUCCESSFUL!")
            
            # Try to download final model
            if test_job.get('final_result_url'):
                print("\n9Ô∏è‚É£ Downloading and verifying final model...")
                resp = requests.get(test_job['final_result_url'])
                
                if resp.status_code == 200:
                    model_buffer = io.BytesIO(resp.content)
                    state_dict = torch.load(model_buffer, map_location='cpu')
                    
                    print(f"   ‚úÖ Final model downloaded!")
                    print(f"   Model keys: {list(state_dict.keys())}")
                    print(f"   Parameters: {sum(p.numel() for p in state_dict.values())}")
                else:
                    print(f"   ‚ö†Ô∏è  Failed to download: {resp.status_code}")
        else:
            print(f"\n‚ö†Ô∏è  Job status: {test_job['status']}")
            print(f"   Expected: COMPLETED")
            print(f"   Completed tasks: {completed_tasks}/5")
    else:
        print("   ‚ùå Job not found")
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä Test Summary")
    print("=" * 60)
    print(f"User created: ‚úÖ")
    print(f"Worker registered: ‚úÖ")
    print(f"Job uploaded: ‚úÖ")
    print(f"CSV split: ‚úÖ")
    print(f"Subtasks completed: {completed_tasks}/5")
    
    if test_job:
        if test_job['status'] == 'COMPLETED' and test_job.get('final_result_url'):
            print(f"Aggregation: ‚úÖ")
            print(f"Final model: ‚úÖ")
            print("\nüéâ FULL END-TO-END TEST PASSED!")
            return True
        else:
            print(f"Aggregation: ‚ùå (Status: {test_job['status']})")
            print("\n‚ö†Ô∏è  Test incomplete - aggregation did not complete")
            return False
    else:
        print("Job status: ‚ùå")
        return False

if __name__ == "__main__":
    try:
        success = main()
        exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
